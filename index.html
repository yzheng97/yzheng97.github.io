<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Yu Zheng</title>
  
  <meta name="author" content="Yu Zheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Yu Zheng</name>
              </p>
              <p>I am currently a Postdoctoral Research Fellow of <a href="http://www.au.tsinghua.edu.cn/publish/auen/index.html"> Department 
                of Automation</a> at <a href="https://www.tsinghua.edu.cn/publish/thu2018en/index.html"> Tsinghua University</a>, affiliated with Intelligent Vision Group (IVG), supervised by 
                Prof.<a href="https://www.au.tsinghua.edu.cn/info/1078/3126.htm"> Jie Zhou</a>. 
			        </p>
        <p>
          I received the B.Eng. and Ph.D. degrees in Control Science and Engineering from Department of Automatjion, Tsinghua University, China, in 2019 and 2024, respectively, supervised by 
          Prof.<a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/index.html"> Jiwen Lu</a>. 
        </p>
			  <p>
          My recent research interests focus on <b>computer vision safety</b> and <b>3D vision</b>. 
        </p>
              <p style="text-align:center">
                <a href="mailto:yu-zheng@tsinghua.edu.cn"> Email </a> &nbsp/&nbsp  
                <a href="https://github.com/yzheng97"> GitHub </a> &nbsp/&nbsp  
		<a href="https://scholar.google.com/citations?user=J4ZIfhwAAAAJ&hl=zh-CN"> Google Scholar </a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <a href="images/yuzheng.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/yuzheng.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
		
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
		  <li style="margin: 5px;">
                <b>2025-11:</b> One paper on Robust 3D Reconstruction is accepted to <a href="https://link.springer.com/journal/11263">IJCV</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2025-06:</b> Two papers on visual forensics are accepted to <a href="https://iccv.thecvf.com/">ICCV 2025</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2025-05:</b> One paper on 3D representation learning and neural rendering is accepted to <a href="https://signalprocessingsociety.org/publications-resources/ieee-transactions-multimedia">TMM</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2025-01:</b> One paper on multi-view 3D object detection is accepted to <a href="https://openreview.net/forum?id=DtFCIfvAFc">ICLR 2025</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2024-09:</b> One paper on 3D recognition is accepted to <a href="https://ieeexplore.ieee.org/document/10666993/">TIP</a>.
              </li>
        <li style="margin: 5px;">
                <b>2024-06:</b> I am honored with Excellent Doctoral Dissertation of Tsinghua University. <b>(top 7.3%)</b>.
              </li>
	      <li style="margin: 5px;">
                <b>2023-11:</b> One paper on 3D object detection is accepted to <a href="https://ieeexplore.ieee.org/document/10330125/">T-PAMI</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2023-05:</b> One paper on semantic neural rendering is accepted to <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Semantic_Ray_Learning_a_Generalizable_Semantic_Field_With_Cross-Reprojection_Attention_CVPR_2023_paper.pdf">CVPR 2023</a>.
              </li>
        <li style="margin: 5px;">
                <b>2022-09:</b> One paper on 3D recognition are accepted to <a href="https://ieeexplore.ieee.org/document/9892683">TIP</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2022-05:</b> Two papers on 3D object detection are accepted to <a href="https://cvpr2022.thecvf.com/">CVPR 2022</a>, one with <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_HyperDet3D_Learning_a_Scene-Conditioned_3D_Object_Detector_CVPR_2022_paper.pdf">oral presentation</a>.
              </li>
	      <li style="margin: 5px;">
                <b>2020-07:</b> One paper on 3D object detection is accepted to <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650460.pdf/">ECCV 2020</a>.
              </li> 
        <li style="margin: 5px;">
                <b>2019-06:</b> I am honored with the Future Scholar Scholarship of Tsinghua University. <b>(top 5%)</b></a>.
              </li>
	      <li style="margin: 5px;">
                <b>2019-03:</b> Two papers on 3D recognition and instructional video analysis are accepted to <a href="http://cvpr2019.thecvf.com/">CVPR 2019</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
	  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
			  <p></p>
            </td>
          </tr>
        </tbody></table>

	(* indicates equal contribution.)
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/opo.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>OPONeRF: One-Point-One NeRF for Robust Few-shot Rendering</papertitle>
              <br>
		<strong>Yu Zheng</strong>, Yueqi Duan, Kangfu Zheng, Hongru Yan, Jiwen Lu and Jie Zhou
              <br>
              <em>International Journal of Computer Vision (IJCV)</em>, 2025
			  <br>
              <a href="https://arxiv.org/abs/2409.20043">[PDF]</a> 
			<a href="https://yzheng97.github.io/OPONeRF">[Project]</a>
	      <a href="https://github.com/yzheng97/OPONeRF">[Code]</a>
		  <a href="https://cloud.tsinghua.edu.cn/d/215adaef3b3f4618b481">[Dataset]</a>
              <br>
              <p>OPONeRF introduces a divide-and-conquer strategy for few-shot NeRFs, adapting to unseen motion and lighting changes by learning a personalized neural renderer for every sampled point. </p><br><br>
            </td>
          </tr>
			
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/cdal.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Counterfactually Decoupled Attention for Open-world Model Attribution</papertitle>
              <br>
		<strong>Yu Zheng*</strong>, Boyang Gong*, Fanye Kong, Yueqi Duan, Bingyao Yu, Wenzhao Zheng, Lei Chen, Jiwen Lu and Jie Zhou
              <br>
              <em>International Conference on Computer Vision (ICCV)</em>, 2025
			  <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2025/papers/Zheng_Learning_Counterfactually_Decoupled_Attention_for_Open-World_Model_Attribution_ICCV_2025_paper.pdf">[PDF]</a> 
				<a href="https://openaccess.thecvf.com/content/ICCV2025/supplemental/Zheng_Learning_Counterfactually_Decoupled_ICCV_2025_supplemental.pdf">[Supp]</a>
	      <a href="https://github.com/yzheng97/CDAL">[Code]</a>
              <br>
              <p>By extracting factual and counterfactual attention maps and maximizing their causal effect, CDAL effectively isolates model-specific artifacts from source content biases, thus enhancing generalization to unseen generative models with minimal computational overhead.</p><br><br>
            </td>
          </tr>

		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/shadownerf.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>ShadowNeRF: Learning Neural Radiance Field with Sight Degradation and Recovery</papertitle>
              <br>
		<strong>Yu Zheng</strong>, Hongru Yan, Yueqi Duan, and Jiwen Lu
              <br>
              <em>IEEE Transactions on Multimedia (TMM)</em>, 2025
			  <br>
              <a href="https://ieeexplore.ieee.org/document/11329474">[PDF]</a> 
              <br>
              <p>We enhance the 3D representational ability of Neural Radiance Field through artificial sight degradation and self-supervised recovery.</p><br><br>
            </td>
          </tr>
	
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/gaussiandet.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection</papertitle>
              <br>
      Hongru Yan*, <strong>Yu Zheng*</strong>, and Yueqi Duan
              <br>
              <em>International Conference on Learning Representations (ICLR)</em>, 2025
			  <br>
              <a href="https://openreview.net/pdf?id=DtFCIfvAFc">[PDF]</a> 
	            <a href="https://yzheng97.github.io/Gaussian-Det/">[Project]</a> 
              <br>
              <p> It is the first 3D Gaussian based 3D object detector, where we address the outlier issue via closure measurement.</p><br><br>
            </td>
          </tr>
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/gsrn.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structural Relation Modeling of 3D Point Clouds</papertitle>
              <br>
		<strong>Yu Zheng</strong>, Jiwen Lu, Yueqi Duan, and Jie Zhou
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2024
			  <br>
              <a href="https://ieeexplore.ieee.org/document/10666993">[PDF]</a> 
              <a href="https://github.com/duanyueqi/SRN">[Code]</a> 
              <br>
              <p>We propose a graph-based 3D point cloud recognition scheme that weighs the contribution of local geometries from an entropy perspective.</p><br><br>
            </td>
          </tr>

  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hyperformer.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Dynamic Scene-Conditioned 3D Object Detectors</papertitle>
              <br>
		<strong>Yu Zheng</strong>, Yueqi Duan, Zongtai Li, Jie Zhou, and Jiwen Lu
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</em>, 2023
			  <br>
              <a href="https://ieeexplore.ieee.org/document/10330125">[PDF]</a> 
              <br>
              <p>We propose to decouple the 3D object detection task into ''what'' and ''where'' as an upstream scene prior .</p><br><br>
            </td>
          </tr>
	
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/sray.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic-Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention</papertitle>
              <br>
		Fangfu Liu, Chubin Zhang, <strong>Yu Zheng</strong>, and Yueqi Duan
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2023
			  <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Semantic_Ray_Learning_a_Generalizable_Semantic_Field_With_Cross-Reprojection_Attention_CVPR_2023_paper.pdf">[PDF]</a> 
              <a href="https://github.com/liuff19/Semantic-Ray">[Code]</a>
              <a href="https://liuff19.github.io/S-Ray">[Project]</a>
              <br>
              <p> It is the first generalizable semantic radiance field, where we tackles the undue multi-view semantic ambiguity by designing a cross-reprojection attention.</p><br><br>
            </td>
          </tr>

	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ras.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PointRas: Uncertainty-Aware Multi-Resolution Learning for Point Cloud Segmentation</papertitle>
              <br>
		<strong>Yu Zheng</strong>, Xiuwei Xu, Jie Zhou, and Jiwen Lu
              <br>
              <em>IEEE Transactions on Image Processing (TIP)</em>, 2022
			  <br>
              <a href="https://ieeexplore.ieee.org/document/9892683/">[PDF]</a> 
              <a href="https://github.com/yzheng97/PointRas">[Code]</a>
              <br>
              <p> We propose to leverage the low-noise representational ability of low-resolution point cloud to regularize the high-noise high-resolution point cloud. </p><br><br>
            </td>
          </tr>
		
	<tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hyperdet.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>HyperDet3D: Learning a Scene-conditioned 3D Object Detector</papertitle>
              <br>
		<strong>Yu Zheng</strong>,Yueqi Duan, Jiwen Lu, Jie Zhou, and Qi Tian
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
			  <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zheng_HyperDet3D_Learning_a_Scene-Conditioned_3D_Object_Detector_CVPR_2022_paper.pdf">[PDF]</a> 
              <br>
              <p> We propose to provide scene prior for 3D objects that share geometry features but are similar in color or texture. </p><br><br>
            </td>
          </tr>
            </p>
          </td>
        </tr>
  <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/br.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
            <papertitle>Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement</papertitle>
            <br>
    Xiuwei Xu, Yifan Wang, <strong>Yu Zheng</strong>,Yongming Rao, Jie Zhou, and Jiwen Lu
            <br>
            <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022
      <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_Back_to_Reality_Weakly-Supervised_3D_Object_Detection_With_Shape-Guided_Label_CVPR_2022_paper.pdf">[PDF]</a> 
            <a href="https://github.com/wyf-ACCEPT/BackToReality">[Code]</a>
            <br>
            <p> We propose to fully utilize the synthetic 3D assets with physics constraint to serve for real-world 3D detection with scarce supervisions. </p><br><br>
          </td>
        </tr>
          </p>
        </td>
      </tr>
  <tr>
        <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/riou.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
          <papertitle>Rotation-robust Intersection over Union for 3D Object Detection</papertitle>
          <br>
<strong>Yu Zheng</strong>, Danyang Zhang, Sinan Xie, Jiwen Lu, and Jie Zhou
          <br>
          <em>European Conference on Computer Vision (ECCV)</em>, 2020
    <br>
          <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650460.pdf">[PDF]</a> 
          <a href="docs/code/riou.py">[Code]</a>
          <br>
          <p> An optimizing method that tackles the loss-metric mismatch in oriented object detection, not limited to 3D scenarios. </p><br><br>
        </td>
      </tr>
        </p>
      </td>
    </tr>

  <tr>
      <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/srn.png" alt="dise">
            </td>
            <td width="75%" valign="center">
        <papertitle>Structural Relational Reasoning of Point Clouds</papertitle>
        <br>
        Yueqi Duan, <strong>Yu Zheng</strong>, Jiwen Lu, Jie Zhou, Qi Tian
        <br>
        <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
  <br>
        <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Duan_Structural_Relational_Reasoning_of_Point_Clouds_CVPR_2019_paper.pdf">[PDF]</a> 
        <a href="https://github.com/duanyueqi/SRN">[Code]</a>
        <br>
        <p> We propose to learn the abundant geometrical and locational relations in 3D point cloud.</p><br><br>
      </td>
    </tr>
      </p>
    </td>
  </tr>

<tr>
    <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/coin.png" alt="dise">
            </td>
            <td width="75%" valign="center">
      <papertitle>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</papertitle>
      <br>
      Yansong Tang, Dajun Ding, Yongmign Rao, <strong>Yu Zheng</strong>, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou
      <br>
      <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2019
<br>
      <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Tang_COIN_A_Large-Scale_Dataset_for_Comprehensive_Instructional_Video_Analysis_CVPR_2019_paper.pdf">[PDF]</a> 
      <a href="https://coin-dataset.github.io/">[Project]</a>
      <a href="https://github.com/coin-dataset/code/">[Evaluation]</a>
      <a href="https://github.com/coin-dataset/annotations/">[Annotations]</a>
      <a href="https://github.com/coin-dataset/annotation-tool/">[Annotation Tool]</a>
      <br>
      <p> We construct by far the largest-scale (>10k) instructional video datasets with abundant annotations.</p><br><br>
    </td>
  </tr>
    </p>
  </td>
</tr>
      </tbody></table>


		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Honors and Awards</heading>
            <p>
              <li style="margin: 5px;" >
                Excellent Doctoral Dissertation of Tsinghua University <b>(top 7.3%)</b>, 2024. 
              </li>
             <li style="margin: 5px;" >
                Future Scholar Scholarship of Tsinghua University <b>(top 5%)</b>, 2019.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

	
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Professional Activities</heading>
            <p>
	      <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
              </li>
        <li style="margin: 5px;" >
                <b>Reviewer,</b> International Conference on Computer Vision (ICCV).
              </li>
	      <li style="margin: 5px;" >
                <b>Reviewer,</b> European Conference on Computer Vision (ECCV).
              </li>
              <li style="margin: 5px;" >
                <b>Reviewer,</b> Annual AAAI Conference on Artificial Intelligence (AAAI).
              </li>
              <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE International Conference on Multimedia and Expo (ICME).
              </li>
             <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI).
              </li>
             <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE Transactions on Image Processing (TIP).
              </li>
             <li style="margin: 5px;" >
                <b>Reviewer,</b>  IEEE Transactions on Information Forensics and Security (TIFS).
              </li>
             <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE Transactions on Multimedia (TMM).
              </li>
             <li style="margin: 5px;" >
                <b>Reviewer,</b> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT).
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>


	
	  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://nvlabs.github.io/face-vid2vid/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
       
      </td>
    </tr>
  </table>
</body>

</html>
